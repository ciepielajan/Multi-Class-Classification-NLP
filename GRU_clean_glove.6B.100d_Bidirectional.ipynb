{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detecting_intentions_MLP",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOIeNlBcL+P4NVETaD2PG4Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ciepielajan/Multi-Class-Classification-NLP/blob/main/GRU_clean_glove.6B.100d_Bidirectional.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsNqh1BptrXp"
      },
      "source": [
        "GRU_clean_glove.6B.100d_Bidirectional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us7PJ6w-n8ge"
      },
      "source": [
        "#### Pobranie danych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ_7ET22CAsA",
        "outputId": "3331b08c-5403-4cc5-d0fd-f7954da95b4c"
      },
      "source": [
        "# https://drive.google.com/file/d/1fI6EXyD9TMTC1jzdu206ljXOGNjdHprq/view?usp=sharing\n",
        "!gdown --id \"1fI6EXyD9TMTC1jzdu206ljXOGNjdHprq\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fI6EXyD9TMTC1jzdu206ljXOGNjdHprq\n",
            "To: /content/user_intent.zip\n",
            "\r  0% 0.00/271k [00:00<?, ?B/s]\r100% 271k/271k [00:00<00:00, 2.50MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWusnzuPCHqv",
        "outputId": "09089ea2-4e41-42e2-a454-180f97e41249"
      },
      "source": [
        "!unzip \"user_intent.zip\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  user_intent.zip\n",
            "  inflating: train.csv               \n",
            "  inflating: __MACOSX/._train.csv    \n",
            "  inflating: validation.csv          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CI1LZgj-Kin"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUKeVYWfCOdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b54a0d8-6c39-4204-ad4c-9773ad7c403d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "data_set_train = pd.read_csv(\"train.csv\")\n",
        "data_set_valid = pd.read_csv(\"validation.csv\")\n",
        "\n",
        "print(data_set_train.shape)\n",
        "print(data_set_valid.shape)\n",
        "\n",
        "print(data_set_train.columns)\n",
        "print(data_set_valid.columns)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13784, 2)\n",
            "(700, 2)\n",
            "Index(['text', 'label'], dtype='object')\n",
            "Index(['text', 'label'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vg1fqbGoCrp"
      },
      "source": [
        "#### Podstawowy process text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GQNuCjSlz0F"
      },
      "source": [
        "import re\n",
        "def process_text(sentence):\n",
        "    sentence = re.sub('[A-Za-z0-9]+@[a-zA-z].[a-zA-Z]+', '', sentence)  # maile\n",
        "    sentence = re.sub('(http[s]*:[/][/])[a-zA-Z0-9]+', '', sentence)  # linki\n",
        "    sentence = re.sub(r\"<[^>]+>\", \" \", sentence) # remove html tag\n",
        "    sentence = re.sub(r\"[^a-zA-Z\\s]\", \"\", sentence)  # remove punctuations and numbers   \n",
        "    sentence = re.sub(r\"\\b[A-Za-z]{1}\\b\", \"\", sentence)  # remove single characters\n",
        "\n",
        "    sentence = re.sub(\"^\\s+|\\s+$\", \"\", sentence, flags=re.UNICODE) # Remove spaces both in the BEGINNING and in the END of a string:\n",
        "    sentence = \" \".join(re.split(\"\\s+\", sentence, flags=re.UNICODE))  # Remove ONLY DUPLICATE spaces:\n",
        "    sentence = sentence.lower()\n",
        "    \n",
        "    return sentence"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZJOju1sFOwp"
      },
      "source": [
        "data_set_train[\"clean_text\"] = data_set_train[\"text\"].apply(lambda x: process_text(x))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7epaW5cXoXef"
      },
      "source": [
        "#### `LabelEncoder` oraz `to_categorical`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "022rNoaL-xbl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "0bfd2bfe-0b8f-4c23-b87a-7fb937360af0"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical \n",
        "\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "data_set_train[\"labelencoder\"] = labelencoder.fit_transform(data_set_train[\"label\"])\n",
        "\n",
        "dummy_y = to_categorical(data_set_train[\"labelencoder\"], dtype =\"float32\")\n",
        "\n",
        "data_set_train[[\"clean_text\",\"label\",\"labelencoder\"]].head(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_text</th>\n",
              "      <th>label</th>\n",
              "      <th>labelencoder</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>find cinema nearest for films</td>\n",
              "      <td>SearchScreeningEvent</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>give the current series two stars</td>\n",
              "      <td>RateBook</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>find the good girl at movie house</td>\n",
              "      <td>SearchScreeningEvent</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>please make reservations for three at kosher t...</td>\n",
              "      <td>BookRestaurant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what is the forecast for here one second from now</td>\n",
              "      <td>GetWeather</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          clean_text  ... labelencoder\n",
              "0                      find cinema nearest for films  ...            6\n",
              "1                  give the current series two stars  ...            4\n",
              "2                  find the good girl at movie house  ...            6\n",
              "3  please make reservations for three at kosher t...  ...            1\n",
              "4  what is the forecast for here one second from now  ...            2\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RfsD1LSpSag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee64e43c-452e-4ac4-deb5-edc535d2e14c"
      },
      "source": [
        "dummy_y.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13784, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4dq1macoeQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb988948-a7a0-4a40-fb6f-ee3bc7347b37"
      },
      "source": [
        "id_intention = 6\n",
        "print(\"Sprawdzenie poprawności LabelEncoder i to_categorical \\n\")\n",
        "print(\"Label - \", data_set_train[\"label\"].iloc[id_intention])\n",
        "print(\"LabelEncoder - \", data_set_train[\"labelencoder\"].iloc[id_intention])\n",
        "print()\n",
        "print(\"to_categorical - \", dummy_y[id_intention])\n",
        "print()\n",
        "print(\"return to LabelEncoder - \",np.argmax(dummy_y[id_intention], axis=-1))\n",
        "print(\"return to Label - \",labelencoder.inverse_transform([np.argmax(dummy_y[id_intention], axis=-1)]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sprawdzenie poprawności LabelEncoder i to_categorical \n",
            "\n",
            "Label -  BookRestaurant\n",
            "LabelEncoder -  1\n",
            "\n",
            "to_categorical -  [0. 1. 0. 0. 0. 0. 0.]\n",
            "\n",
            "return to LabelEncoder -  1\n",
            "return to Label -  ['BookRestaurant']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lZTDFgGoSmT"
      },
      "source": [
        "#### Zdefiniowanie X i y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWd9mEjfBAS2"
      },
      "source": [
        "X = data_set_train[\"clean_text\"]\n",
        "y = dummy_y"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jUlcX2npzFs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9229c8ca-96e4-43f8-beef-107ddf38eb48"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((13784,), (13784, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mS_l8wZoO2R"
      },
      "source": [
        "#### Podział zbioru "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A18D02x5sjFd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_xH9XIGuYCG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ba7119-44ec-419d-b9c5-c6a21f08ad6c"
      },
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11027,), (2757,), (11027, 7), (2757, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DR0gkd1_Uv0"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chWlDg9Mtmya"
      },
      "source": [
        "#### `Tokenizer` i `pad_sequences`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCm-rbaus6QG"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-ihmlc-s6QG"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "maxlen = 13\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X_train = pad_sequences(X_train, padding=\"post\", truncating=\"post\", maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding=\"post\", truncating=\"post\", maxlen=maxlen)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1-W369vaxPo"
      },
      "source": [
        "#### Pobranie Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXlya8aOa1d_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb0bf7a-0e51-4cf9-e847-2c3f4c8fb2b9"
      },
      "source": [
        "!gdown \"http://nlp.stanford.edu/data/glove.6B.zip\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: http://nlp.stanford.edu/data/glove.6B.zip\n",
            "To: /content/glove.6B.zip\n",
            " 71% 609M/862M [01:52<00:48, 5.26MB/s]Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/gdown\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/gdown/cli.py\", line 61, in main\n",
            "    quiet=args.quiet,\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/gdown/download.py\", line 101, in download\n",
            "    for chunk in res.iter_content(chunk_size=CHUNK_SIZE):\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/requests/models.py\", line 751, in generate\n",
            "    for chunk in self.raw.stream(chunk_size, decode_content=True):\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/urllib3/response.py\", line 496, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/urllib3/response.py\", line 444, in read\n",
            "    data = self._fp.read(amt)\n",
            "  File \"/usr/lib/python2.7/httplib.py\", line 611, in read\n",
            "    s = self.fp.read(amt)\n",
            "  File \"/usr/lib/python2.7/socket.py\", line 384, in read\n",
            "    data = self._sock.recv(left)\n",
            "KeyboardInterrupt\n",
            " 71% 612M/862M [01:53<00:46, 5.41MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_AltESOjY3d",
        "outputId": "14888c2f-3283-48df-8ce8-088962d125b0"
      },
      "source": [
        "!unzip \"glove.6B.zip\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open glove.6B.zip, glove.6B.zip.zip or glove.6B.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZKjZoqJXw4F",
        "outputId": "a1da2fdc-3e8c-4f23-b099-aa2af846ec18"
      },
      "source": [
        "!head -2 \"glove.6B.100d.txt\""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "head: cannot open 'glove.6B.100d.txt' for reading: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNM1TtFvbLzp"
      },
      "source": [
        "#### Utworzenie Embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VJ63sXjhY26"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqz69PLnheBy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "c8b39cd7-1b33-4b78-97c4-5c7f5ff2a3ff"
      },
      "source": [
        "embedding_dim = 100\n",
        "embedding_matrix = create_embedding_matrix('glove.6B.100d.txt', tokenizer.word_index, embedding_dim) # glove.twitter.27B.100d"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-bbdd0b48b96b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_embedding_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove.6B.100d.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# glove.twitter.27B.100d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-16454bc9852e>\u001b[0m in \u001b[0;36mcreate_embedding_matrix\u001b[0;34m(filepath, word_index, embedding_dim)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.100d.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHakANxM9lOb"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voT_0sglDTN7"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import GRU, Dense\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlehhRzS_ADA"
      },
      "source": [
        "`Bidirectional` / `dwukierunkowe warstwy rekurencyjne`\n",
        "\n",
        "warstwy przedstawiające te same informacje w sieci rekurencyjnej, ale robiące to na różne sposoby; rozwiązanie to zwiększa dokładność i rozwiązuje problemy z ginięciem informacji"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQbBTgv5-iXp"
      },
      "source": [
        "from keras.layers import Bidirectional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIvlVQnasO3C"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100,  \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen))\n",
        "model.add(Bidirectional(GRU(128)))\n",
        "model.add(Dense(7, activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "print(model.summary())\n",
        "\n",
        "EarlyStop = EarlyStopping(monitor='val_loss', \n",
        "                          patience=5,\n",
        "                          verbose=1)\n",
        "\n",
        "history = model.fit(X_train, \n",
        "                    y_train, \n",
        "                    epochs=20, \n",
        "                    batch_size=64, \n",
        "                    validation_split=0.2, \n",
        "                    callbacks = [EarlyStop] )\n",
        "scores = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy {scores[1] * 100}\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHv6PlrGrYUr"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Dokladnosc trenowania')\n",
        "plt.plot(epochs, val_acc, 'b', label='Dokladnosc walidacji')\n",
        "plt.title('Dokladnosc trenowania i walidacji')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Strata trenowania')\n",
        "plt.plot(epochs, val_loss, 'b', label='Strata walidacji')\n",
        "plt.title('Strata trenowania i walidacji')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui9nW3K0_0bf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJtnYTgA_06S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0IdSKiJtzgd"
      },
      "source": [
        "#### Predykcja na zbiorze validacyjnym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27dr_LCO_1IW"
      },
      "source": [
        "# oczysczenie danych\n",
        "data_set_valid[\"clean_text\"] = data_set_valid[\"text\"].apply(lambda x: process_text(x))\n",
        "\n",
        "# labelencoder \n",
        "data_set_valid[\"labelencoder\"] = labelencoder.fit_transform(data_set_valid[\"label\"])\n",
        "\n",
        "# tokenizacja weg przetrenowanego już tokenizera\n",
        "X_validate = tokenizer.texts_to_sequences(data_set_valid[\"clean_text\"])\n",
        "\n",
        "# pad sequel\n",
        "X_validate = pad_sequences(X_validate, padding=\"post\", truncating=\"post\", maxlen=maxlen)\n",
        "X_validate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOPVqktyANNF"
      },
      "source": [
        "dummy_y_valid = data_set_valid[\"labelencoder\"].values\n",
        "dummy_y_valid[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAiFC304APqL"
      },
      "source": [
        "# Sprawdzenie rozmiaru zbiorów validacyjnego\n",
        "X_validate.shape, dummy_y_valid.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRbEdb7AAXGy"
      },
      "source": [
        "predicted_lstm_val = np.argmax(model.predict(X_validate), axis=-1)\n",
        "predicted_lstm_val[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5wZwn-_uKE9"
      },
      "source": [
        "#### Rozkodowanie przewidzianych i prawidłowych etykiet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4U9hYvnBJ9F"
      },
      "source": [
        "y_pred = labelencoder.inverse_transform(predicted_lstm_val)\n",
        "y_pred = pd.Series(y_pred)\n",
        "y_pred.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW03pUanA729"
      },
      "source": [
        "y_val = labelencoder.inverse_transform(dummy_y_valid)\n",
        "y_val = pd.Series(y_val)\n",
        "y_val.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcGfyRgQudTZ"
      },
      "source": [
        "#### `Confusion matrix`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXVwswIsA9RT"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGr6-cjzBFLc"
      },
      "source": [
        "classes = np.unique(y_val)\n",
        "\n",
        "print('Accuracy:', round(accuracy_score(y_val, y_pred),2))\n",
        "print('F1_score:', round(f1_score(y_val, y_pred, average='weighted'),2))\n",
        "\n",
        "print(classification_report(y_val, y_pred))\n",
        "# Plot confusion matrix\n",
        "cm = confusion_matrix(y_val, y_pred)\n",
        "fig, ax = plt.subplots()\n",
        "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, cbar=False)\n",
        "ax.set(xlabel='Pred', ylabel='True', xticklabels=classes, yticklabels=classes, title='Confusion matrix')\n",
        "plt.yticks(rotation=0)\n",
        "plt.xticks(rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uxfqF9uum32"
      },
      "source": [
        "#### Zapoznanie się z błędnymi predykcjami "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28QYnOPM-mJz"
      },
      "source": [
        "indexes = []\n",
        "for i, phrase in enumerate(y_pred):\n",
        "  if phrase == 'SearchCreativeWork':\n",
        "    if y_val[i] == 'SearchScreeningEvent':\n",
        "      indexes.append(i)\n",
        "print(indexes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouy1yzy8-048"
      },
      "source": [
        "for i in indexes:\n",
        "  print(f\"----------------------------\\nTekst komendy:\\n{data_set_valid['text'][i]}\")\n",
        "  print(f\"Oczyszczona komenda:\\n{data_set_valid['clean_text'][i]}\")\n",
        "  print(f'True category: {y_val[i]}')\n",
        "  print(f'Predicted category: {y_pred[i]}')\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm61MY9CSpLm"
      },
      "source": [
        "Wnioski:\n",
        ">  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va8xf987tkbG"
      },
      "source": [
        "> Wyraźnie widać że warstwa Bidirectional poradziła sobie znacznie lepiej na dotychczas problematycznym przykładzie True category: SearchScreeningEvent i \n",
        "Predicted category: SearchCreativeWork. Model GRU Bidirectional popełnij tylko jeden błąd przy tych etykietach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1P7YIZyuCSX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}