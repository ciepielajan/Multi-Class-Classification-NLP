{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-MultinomialNB_TfIdf_Chi-Square.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbL5TvtOFqqbYFlxRRchPL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ciepielajan/Multi-Class-Classification-NLP/blob/main/ML-MultinomialNB%20TfIdf%20feature%20selection%20Chi-Square.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeNbiQPho0OC"
      },
      "source": [
        "ML-MultinomialNB TfIdf feature selection Chi-Square "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us7PJ6w-n8ge"
      },
      "source": [
        "#### Pobranie danych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ_7ET22CAsA",
        "outputId": "6641a918-2251-4c42-9c8d-fd1c24639a08"
      },
      "source": [
        "# https://drive.google.com/file/d/1fI6EXyD9TMTC1jzdu206ljXOGNjdHprq/view?usp=sharing\n",
        "!gdown --id \"1fI6EXyD9TMTC1jzdu206ljXOGNjdHprq\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fI6EXyD9TMTC1jzdu206ljXOGNjdHprq\n",
            "To: /content/user_intent.zip\n",
            "\r  0% 0.00/271k [00:00<?, ?B/s]\r100% 271k/271k [00:00<00:00, 39.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWusnzuPCHqv",
        "outputId": "718a8932-a3fd-42a7-9191-88a9e2af5213"
      },
      "source": [
        "!unzip \"user_intent.zip\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  user_intent.zip\n",
            "  inflating: train.csv               \n",
            "  inflating: __MACOSX/._train.csv    \n",
            "  inflating: validation.csv          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CI1LZgj-Kin"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUKeVYWfCOdU",
        "outputId": "371788f3-47f4-4773-8ffe-04ceadb49a84"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "data_set_train = pd.read_csv(\"train.csv\")\n",
        "data_set_valid = pd.read_csv(\"validation.csv\")\n",
        "\n",
        "print(data_set_train.shape)\n",
        "print(data_set_valid.shape)\n",
        "\n",
        "print(data_set_train.columns)\n",
        "print(data_set_valid.columns)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13784, 2)\n",
            "(700, 2)\n",
            "Index(['text', 'label'], dtype='object')\n",
            "Index(['text', 'label'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vg1fqbGoCrp"
      },
      "source": [
        "#### Podstawowy process text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GQNuCjSlz0F"
      },
      "source": [
        "import re\n",
        "def process_text(sentence):\n",
        "    sentence = re.sub('[A-Za-z0-9]+@[a-zA-z].[a-zA-Z]+', '', sentence)  # maile\n",
        "    sentence = re.sub('(http[s]*:[/][/])[a-zA-Z0-9]+', '', sentence)  # linki\n",
        "    sentence = re.sub(r\"<[^>]+>\", \" \", sentence) # remove html tag\n",
        "    sentence = re.sub(r\"[^a-zA-Z\\s]\", \"\", sentence)  # remove punctuations and numbers   \n",
        "    sentence = re.sub(r\"\\b[A-Za-z]{1}\\b\", \"\", sentence)  # remove single characters\n",
        "\n",
        "    sentence = re.sub(\"^\\s+|\\s+$\", \"\", sentence, flags=re.UNICODE) # Remove spaces both in the BEGINNING and in the END of a string:\n",
        "    sentence = \" \".join(re.split(\"\\s+\", sentence, flags=re.UNICODE))  # Remove ONLY DUPLICATE spaces:\n",
        "    sentence = sentence.lower()\n",
        "    \n",
        "    return sentence"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZJOju1sFOwp"
      },
      "source": [
        "data_set_train[\"clean_text\"] = data_set_train[\"text\"].apply(lambda x: process_text(x))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7epaW5cXoXef"
      },
      "source": [
        "#### `LabelEncoder` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "022rNoaL-xbl"
      },
      "source": [
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# labelencoder = LabelEncoder()\n",
        "# data_set_train[\"labelencoder\"] = labelencoder.fit_transform(data_set_train[\"label\"])\n",
        "\n",
        "# # dummy_y = to_categorical(data_set_train[\"labelencoder\"], dtype =\"float32\")\n",
        "\n",
        "# data_set_train[[\"clean_text\",\"label\",\"labelencoder\"]].head(5)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4dq1macoeQC"
      },
      "source": [
        "# id_intention = 6\n",
        "# print(\"Sprawdzenie poprawności LabelEncoder i to_categorical \\n\")\n",
        "# print(\"Label - \", data_set_train[\"label\"].iloc[id_intention])\n",
        "# print(\"LabelEncoder - \", data_set_train[\"labelencoder\"].iloc[id_intention])\n",
        "# print(\"return to Label - \", labelencoder.inverse_transform([ data_set_train[\"labelencoder\"].iloc[id_intention]  ]) )"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lZTDFgGoSmT"
      },
      "source": [
        "#### Zdefiniowanie X i y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWd9mEjfBAS2"
      },
      "source": [
        "X = data_set_train[\"clean_text\"]\n",
        "y = data_set_train[\"label\"]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jUlcX2npzFs",
        "outputId": "92b442a7-37a0-4741-8a2b-3c372db0b5d5"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((13784,), (13784,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mS_l8wZoO2R"
      },
      "source": [
        "#### Podział zbioru "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A18D02x5sjFd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_xH9XIGuYCG",
        "outputId": "858ac947-0fdc-47da-bc00-0bfa355b79fd"
      },
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11027,), (2757,), (11027,), (2757,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chWlDg9Mtmya"
      },
      "source": [
        "#### `TfIdfVectorizer`\n",
        "\n",
        "W pierwszej kolejności analizie poddamy przekształcenie korpusu oczyszczonych danych za pomocą TfIdfVectorizera w celu wydobycia słownika najczęstych słów i stowrzenia macierzy cech - złożonej ze słów \"kluczy\" w tekstach komend poprzez przypisanie im stosownej wagi. Słowa będące najistotniejsze będą miały najwyższe wagi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtY8m6FQuvVz"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft268rQJwxmH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05daae58-a2f7-47cd-80b0-1b3117df7b40"
      },
      "source": [
        "# stworzenie TfIdfVectorizera biorącego max 10000 najczęstszych słów ze wszystkich tekstów, zbudowanego z unigramów i bigramów\n",
        "vectorizer_tf_idf = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
        "vectorizer_tf_idf.fit(X_train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=10000,\n",
              "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_-JzIYZIDya"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4-CscIMygJ5"
      },
      "source": [
        "W celu wyciągnięcia najbardziej istotnych cech (z dotychczas zdefiniowanych jako max_features=10000) wykonamy następujące operacje:\n",
        "\n",
        "\n",
        "1. potraktujemy każdą z kategorii jako binarną (np. 'RateBook' będzie 1 a pozostałe 0),\n",
        "2. przeprowadzimy [Chi-Square](https://en.wikipedia.org/wiki/Chi-squared_test) test do oceny czy dana cecha i binarny target są niezależne,\n",
        "3. pozostawimy tylko cechy z określonym p-value z Chi-Square test.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M34ozQD1pag"
      },
      "source": [
        "from sklearn import feature_selection"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur60XJ6V1GEz",
        "outputId": "37dea4b7-6e5c-473e-b0af-e704768f08ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "y = y_train\n",
        "X_names = vectorizer_tf_idf.get_feature_names()\n",
        "p_value_limit = 0.95\n",
        "\n",
        "dtf_features = pd. DataFrame()\n",
        "for cat in np.unique(y):\n",
        "  chi2, p = feature_selection.chi2(X_train, y==cat)\n",
        "  dtf_features = dtf_features.append(pd.DataFrame(\n",
        "      {'feature': X_names, 'score': 1-p, 'y': cat}\n",
        "  ))\n",
        "  dtf_features = dtf_features.sort_values(['y', 'score'], ascending=[True, False])\n",
        "  dtf_features = dtf_features[dtf_features['score']>p_value_limit]\n",
        "X_names = dtf_features['feature'].unique().tolist()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-7238e5241a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdtf_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mchi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchi2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   dtf_features = dtf_features.append(pd.DataFrame(\n\u001b[1;32m      9\u001b[0m       \u001b[0;34m{\u001b[0m\u001b[0;34m'feature'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'score'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py\u001b[0m in \u001b[0;36mchi2\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;31m# XXX: we might want to do some of the following in logspace instead for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;31m# numerical stability.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    795\u001b[0m               dtype='datetime64[ns]')\n\u001b[1;32m    796\u001b[0m         \"\"\"\n\u001b[0;32m--> 797\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/arrays/numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'book table at brasserie type restaurant that serves jain for party of'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngeQsNFo3kr-"
      },
      "source": [
        "# sprawdzenie zredukowanej liczby cech z 10000\n",
        "len(X_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idast9in3twe"
      },
      "source": [
        "Statystycznie 5947 cech zostało uznanych za wysoce znaczące."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btkmraU239lK"
      },
      "source": [
        "Wyświetlmy dla przykładu kilka z nich dla poszczególnych kategorii czynności:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ2t36p34Gzq"
      },
      "source": [
        "for cat in np.unique(y): \n",
        "  print('# {}:'.format(cat))\n",
        "  print(' -> ilość wybranych cech:',\n",
        "        len(dtf_features[dtf_features['y']==cat]))\n",
        "  print(' -> najpopularniejsze cechy: ',', '.join(\n",
        "      dtf_features[dtf_features['y']==cat]['feature'].values[:10]\n",
        "  ))\n",
        "  print('  ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHs-SwTc4oMY"
      },
      "source": [
        "Zrefitujmy TfIdfVectorizer do nowego zbioru istotnych cech w celu wyprodukowania mniejszej macierzy cech i krótszego słownika."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvT2MGAKIDv1"
      },
      "source": [
        "vectorizer_tf_idf = TfidfVectorizer(vocabulary=X_names)\n",
        "vectorizer_tf_idf.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b2R-FiwxvWm"
      },
      "source": [
        "X_train = vectorizer_tf_idf.transform(X_train)\n",
        "X_test = vectorizer_tf_idf.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyUsnZ6QzrYu"
      },
      "source": [
        "# sprawdzenie rozmiaru X_train\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZCgCSQjGjLp"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AzZtmPuCPTm"
      },
      "source": [
        "X_train.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD_pEHn4Gpdj"
      },
      "source": [
        "pd.DataFrame(X_train.toarray(), columns=vectorizer_tf_idf.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbeVSdurn-Ap"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PgxQ5NkCYbW"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ASHQgFBD0Uh"
      },
      "source": [
        "model = MultinomialNB()\n",
        "\n",
        "# train classifier\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# test\n",
        "predicted = model.predict(X_test)\n",
        "predicted_prob = model.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHaNraBLA-EX"
      },
      "source": [
        "predicted_prob[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKBpQCBuIzXb"
      },
      "source": [
        "accuracy_score(y_test, predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsF8Uy7YIzU5"
      },
      "source": [
        "confusion_matrix(y_test, predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HonM11JIBnb_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYy5ee7RBdub"
      },
      "source": [
        "classes = np.unique(y_test)\n",
        "\n",
        "print('Accuracy:', round(accuracy_score(y_test, predicted),2))\n",
        "print('F1_score:', round(f1_score(y_test, predicted, average='weighted'),2))\n",
        "\n",
        "print(classification_report(y_test, predicted))\n",
        "# Plot confusion matrix\n",
        "cm = confusion_matrix(y_test, predicted)\n",
        "fig, ax = plt.subplots()\n",
        "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, cbar=False)\n",
        "ax.set(xlabel='Pred', ylabel='True', xticklabels=classes, yticklabels=classes, title='Confusion matrix')\n",
        "plt.yticks(rotation=0)\n",
        "plt.xticks(rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kqg1fNcqBdr_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1HxryC1BdpJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO_i7ldVBdlU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieuYW1RPIzSJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0IdSKiJtzgd"
      },
      "source": [
        "#### Predykcja na zbiorze validacyjnym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27dr_LCO_1IW"
      },
      "source": [
        "# oczysczenie danych\n",
        "data_set_valid[\"clean_text\"] = data_set_valid[\"text\"].apply(lambda x: process_text(x))\n",
        "\n",
        "# # labelencoder \n",
        "# data_set_valid[\"labelencoder\"] = labelencoder.fit_transform(data_set_valid[\"label\"])\n",
        "\n",
        "X_validate = vectorizer_tf_idf.transform(data_set_valid[\"clean_text\"])\n",
        "\n",
        "X_validate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOPVqktyANNF"
      },
      "source": [
        "y_val = data_set_valid[\"label\"].values\n",
        "y_val[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAiFC304APqL"
      },
      "source": [
        "# Sprawdzenie rozmiaru zbiorów validacyjnego\n",
        "X_validate.shape, y_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRbEdb7AAXGy"
      },
      "source": [
        "predicted_validate = model.predict(X_validate)\n",
        "predicted_prob_validate = model.predict_proba(X_validate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcGfyRgQudTZ"
      },
      "source": [
        "#### `Confusion matrix`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXVwswIsA9RT"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGr6-cjzBFLc"
      },
      "source": [
        "classes = np.unique(y_val)\n",
        "\n",
        "print('Accuracy:', round(accuracy_score(y_val, predicted_validate),2))\n",
        "print('F1_score:', round(f1_score(y_val, predicted_validate, average='weighted'),2))\n",
        "\n",
        "print(classification_report(y_val, predicted_validate))\n",
        "# Plot confusion matrix\n",
        "cm = confusion_matrix(y_val, predicted_validate)\n",
        "fig, ax = plt.subplots()\n",
        "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, cbar=False)\n",
        "ax.set(xlabel='Pred', ylabel='True', xticklabels=classes, yticklabels=classes, title='Confusion matrix')\n",
        "plt.yticks(rotation=0)\n",
        "plt.xticks(rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uxfqF9uum32"
      },
      "source": [
        "#### Zapoznanie się z błędnymi predykcjami "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28QYnOPM-mJz"
      },
      "source": [
        "indexes = []\n",
        "for i, phrase in enumerate(predicted_validate):\n",
        "  if phrase == 'SearchCreativeWork':\n",
        "    if y_val[i] == 'SearchScreeningEvent':\n",
        "      indexes.append(i)\n",
        "print(indexes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouy1yzy8-048"
      },
      "source": [
        "# Tekst komendy użytkownika\n",
        "for i in indexes:\n",
        "  print(f\"----------------------------\\nTekst komendy:\\n{data_set_valid['text'][i]}\")\n",
        "  print(f'True category: {y_val[i]}')\n",
        "  print(f'Predicted category: {predicted_validate[i]}')\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm61MY9CSpLm"
      },
      "source": [
        "Wnioski:\n",
        "> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxsrsxoBn4lP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
