{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detecting_intentions_MLP",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCMRvMZrWTkDVskk5OAC2F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ciepielajan/Multi-Class-Classification-NLP/blob/main/Detecting_intentions_CNN_KerasClassifier_i_RandomizedSearchCV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us7PJ6w-n8ge"
      },
      "source": [
        "#### Pobranie danych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ_7ET22CAsA",
        "outputId": "69fda341-4e4b-4a92-cc04-9e522a8967ee"
      },
      "source": [
        "# https://drive.google.com/file/d/1fI6EXyD9TMTC1jzdu206ljXOGNjdHprq/view?usp=sharing\n",
        "!gdown --id \"1fI6EXyD9TMTC1jzdu206ljXOGNjdHprq\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fI6EXyD9TMTC1jzdu206ljXOGNjdHprq\n",
            "To: /content/user_intent.zip\n",
            "\r  0% 0.00/271k [00:00<?, ?B/s]\r100% 271k/271k [00:00<00:00, 29.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWusnzuPCHqv",
        "outputId": "cdba385a-6fdd-44a2-ca2d-700af33afd5f"
      },
      "source": [
        "!unzip \"user_intent.zip\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  user_intent.zip\n",
            "  inflating: train.csv               \n",
            "  inflating: __MACOSX/._train.csv    \n",
            "  inflating: validation.csv          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CI1LZgj-Kin"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUKeVYWfCOdU",
        "outputId": "caddb737-0c95-450d-f175-dd88e7c22393"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "data_set_train = pd.read_csv(\"train.csv\")\n",
        "data_set_valid = pd.read_csv(\"validation.csv\")\n",
        "\n",
        "print(data_set_train.shape)\n",
        "print(data_set_valid.shape)\n",
        "\n",
        "print(data_set_train.columns)\n",
        "print(data_set_valid.columns)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13784, 2)\n",
            "(700, 2)\n",
            "Index(['text', 'label'], dtype='object')\n",
            "Index(['text', 'label'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vg1fqbGoCrp"
      },
      "source": [
        "#### Podstawowy process text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GQNuCjSlz0F"
      },
      "source": [
        "import re\n",
        "def process_text(sentence):\n",
        "    sentence = re.sub('[A-Za-z0-9]+@[a-zA-z].[a-zA-Z]+', '', sentence)  # maile\n",
        "    sentence = re.sub('(http[s]*:[/][/])[a-zA-Z0-9]+', '', sentence)  # linki\n",
        "    sentence = re.sub(r\"<[^>]+>\", \" \", sentence) # remove html tag\n",
        "    sentence = re.sub(r\"[^a-zA-Z\\s]\", \"\", sentence)  # remove punctuations and numbers   \n",
        "    sentence = re.sub(r\"\\b[A-Za-z]{1}\\b\", \"\", sentence)  # remove single characters\n",
        "\n",
        "    sentence = re.sub(\"^\\s+|\\s+$\", \"\", sentence, flags=re.UNICODE) # Remove spaces both in the BEGINNING and in the END of a string:\n",
        "    sentence = \" \".join(re.split(\"\\s+\", sentence, flags=re.UNICODE))  # Remove ONLY DUPLICATE spaces:\n",
        "    sentence = sentence.lower()\n",
        "    \n",
        "    return sentence"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZJOju1sFOwp"
      },
      "source": [
        "data_set_train[\"clean_text\"] = data_set_train[\"text\"].apply(lambda x: process_text(x))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7epaW5cXoXef"
      },
      "source": [
        "#### `LabelEncoder` oraz `to_categorical`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "022rNoaL-xbl",
        "outputId": "0803d530-405a-422c-9d66-7052fa23eb41"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical \n",
        "\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "data_set_train[\"labelencoder\"] = labelencoder.fit_transform(data_set_train[\"label\"])\n",
        "\n",
        "dummy_y = to_categorical(data_set_train[\"labelencoder\"], dtype =\"float32\")\n",
        "\n",
        "data_set_train[[\"clean_text\",\"label\",\"labelencoder\"]].head(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_text</th>\n",
              "      <th>label</th>\n",
              "      <th>labelencoder</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>find cinema nearest for films</td>\n",
              "      <td>SearchScreeningEvent</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>give the current series two stars</td>\n",
              "      <td>RateBook</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>find the good girl at movie house</td>\n",
              "      <td>SearchScreeningEvent</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>please make reservations for three at kosher t...</td>\n",
              "      <td>BookRestaurant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what is the forecast for here one second from now</td>\n",
              "      <td>GetWeather</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          clean_text  ... labelencoder\n",
              "0                      find cinema nearest for films  ...            6\n",
              "1                  give the current series two stars  ...            4\n",
              "2                  find the good girl at movie house  ...            6\n",
              "3  please make reservations for three at kosher t...  ...            1\n",
              "4  what is the forecast for here one second from now  ...            2\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RfsD1LSpSag",
        "outputId": "6d82bce0-f763-4cfb-e950-fbe163f3861c"
      },
      "source": [
        "dummy_y.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13784, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4dq1macoeQC",
        "outputId": "4df4a2b6-faed-4ab9-e469-2d4ed76db479"
      },
      "source": [
        "id_intention = 6\n",
        "print(\"Sprawdzenie poprawności LabelEncoder i to_categorical \\n\")\n",
        "print(\"Label - \", data_set_train[\"label\"].iloc[id_intention])\n",
        "print(\"LabelEncoder - \", data_set_train[\"labelencoder\"].iloc[id_intention])\n",
        "print()\n",
        "print(\"to_categorical - \", dummy_y[id_intention])\n",
        "print()\n",
        "print(\"return to LabelEncoder - \",np.argmax(dummy_y[id_intention], axis=-1))\n",
        "print(\"return to Label - \",labelencoder.inverse_transform([np.argmax(dummy_y[id_intention], axis=-1)]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sprawdzenie poprawności LabelEncoder i to_categorical \n",
            "\n",
            "Label -  BookRestaurant\n",
            "LabelEncoder -  1\n",
            "\n",
            "to_categorical -  [0. 1. 0. 0. 0. 0. 0.]\n",
            "\n",
            "return to LabelEncoder -  1\n",
            "return to Label -  ['BookRestaurant']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lZTDFgGoSmT"
      },
      "source": [
        "#### Zdefiniowanie X i y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWd9mEjfBAS2"
      },
      "source": [
        "X = data_set_train[\"clean_text\"]\n",
        "y = dummy_y"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jUlcX2npzFs",
        "outputId": "4a5ac9ce-f457-41e0-adef-dfeac922381e"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((13784,), (13784, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mS_l8wZoO2R"
      },
      "source": [
        "#### Podział zbioru "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A18D02x5sjFd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_xH9XIGuYCG",
        "outputId": "b401ee68-0607-4450-a65a-fe84f9648d89"
      },
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11027,), (2757,), (11027, 7), (2757, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DR0gkd1_Uv0"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chWlDg9Mtmya"
      },
      "source": [
        "#### `Tokenizer` i `pad_sequences`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCm-rbaus6QG"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-ihmlc-s6QG"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "maxlen = 7\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X_train = pad_sequences(X_train, padding=\"post\", truncating=\"post\", maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding=\"post\", truncating=\"post\", maxlen=maxlen)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UAQ4RNONmda"
      },
      "source": [
        "\n",
        "#### `KerasClassifier` i `RandomizedSearchCV`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaRWehbu_VCe"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwih4PgGsS9k"
      },
      "source": [
        "from keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPooling1D, Dropout, Conv1D"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTL6-X08cIcG"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOcctLFPuKoa"
      },
      "source": [
        "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "    model.add(Conv1D(num_filters, kernel_size, activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    # model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(7, activation=\"softmax\")) # sigmoid / softmax\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4tXfrhOxi7E"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fOG5tkSuR0A",
        "outputId": "632fa00c-05e2-453f-93dc-1ce0ebd5ffcf"
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Main settings\n",
        "epochs = 20\n",
        "embedding_dim = 100\n",
        "maxlen = 7   #100   #7\n",
        "output_file = 'output.txt'  # data/\n",
        "\n",
        "# Run grid search for each source (yelp, amazon, imdb)\n",
        "# for source, frame in df.groupby('source'):\n",
        "# print('Running grid search for data set :', source)\n",
        "sentences = data_set_train[\"clean_text\"].values  \n",
        "y = dummy_y\n",
        "# sentences = df['sentence'].values\n",
        "# y = df['label'].values\n",
        "\n",
        "# Train-test split\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "# Tokenize words\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "# Adding 1 because of reserved 0 index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Pad sequences with zeros\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Parameter grid for grid search\n",
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                  kernel_size=[3, 5, 7],\n",
        "                  vocab_size=[vocab_size],\n",
        "                  embedding_dim=[embedding_dim],\n",
        "                  maxlen=[maxlen])\n",
        "model = KerasClassifier(build_fn=create_model,\n",
        "                        epochs=epochs, batch_size=64,\n",
        "                        verbose=False)\n",
        "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=4, verbose=10, n_iter=5)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate testing set\n",
        "test_accuracy = grid.score(X_test, y_test)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=7, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=7, kernel_size=3, embedding_dim=100, score=0.948, total=  26.4s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=7, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   26.4s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=7, kernel_size=3, embedding_dim=100, score=0.958, total=  26.6s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=7, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   53.0s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=7, kernel_size=3, embedding_dim=100, score=0.961, total=  26.1s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=7, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.3min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=7, kernel_size=3, embedding_dim=100, score=0.959, total=  25.7s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=7, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  1.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=128, maxlen=7, kernel_size=3, embedding_dim=100, score=0.950, total=  29.8s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=7, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=128, maxlen=7, kernel_size=3, embedding_dim=100, score=0.959, total=  29.9s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=7, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  2.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=128, maxlen=7, kernel_size=3, embedding_dim=100, score=0.967, total=  29.4s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=7, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  3.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=128, maxlen=7, kernel_size=3, embedding_dim=100, score=0.959, total=  29.8s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=7, kernel_size=7, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  3.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=64, maxlen=7, kernel_size=7, embedding_dim=100, score=0.950, total=  26.0s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=7, kernel_size=7, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  4.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=64, maxlen=7, kernel_size=7, embedding_dim=100, score=0.961, total=  26.3s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=7, kernel_size=7, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=64, maxlen=7, kernel_size=7, embedding_dim=100, score=0.963, total=  26.2s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=7, kernel_size=7, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=64, maxlen=7, kernel_size=7, embedding_dim=100, score=0.962, total=  26.5s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=7, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=7, kernel_size=5, embedding_dim=100, score=0.945, total=  25.6s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=7, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=7, kernel_size=5, embedding_dim=100, score=0.961, total=  25.8s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=7, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=7, kernel_size=5, embedding_dim=100, score=0.956, total=  26.0s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=7, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=7, kernel_size=5, embedding_dim=100, score=0.955, total=  25.7s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=7, kernel_size=7, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=7, kernel_size=7, embedding_dim=100, score=0.945, total=  24.9s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=7, kernel_size=7, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=7, kernel_size=7, embedding_dim=100, score=0.961, total=  24.8s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=7, kernel_size=7, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=7, kernel_size=7, embedding_dim=100, score=0.960, total=  24.7s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=7, kernel_size=7, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=7, kernel_size=7, embedding_dim=100, score=0.964, total=  24.6s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  8.8min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VywFjwoxthN",
        "outputId": "b5afb62e-9485-4dea-8eca-f34b20feef18"
      },
      "source": [
        "grid_result.best_score_"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.959083691239357"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCzI5a2Lxvef",
        "outputId": "e0b14619-97ca-47f3-f14b-1718aa24276a"
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'embedding_dim': 100,\n",
              " 'kernel_size': 7,\n",
              " 'maxlen': 7,\n",
              " 'num_filters': 64,\n",
              " 'vocab_size': 9462}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pmvScCOdj36"
      },
      "source": [
        "# {'embedding_dim': 100,\n",
        "#  'kernel_size': 7,\n",
        "#  'maxlen': 7,\n",
        "#  'num_filters': 64,\n",
        "#  'vocab_size': 9462}\n",
        "# 0.9552150815725327"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChGFmtkZdj1G"
      },
      "source": [
        "# {'embedding_dim': 100,\n",
        "#  'kernel_size': 3,\n",
        "#  'maxlen': 15,\n",
        "#  'num_filters': 64,\n",
        "#  'vocab_size': 9462}\n",
        "# 0.9813313335180283"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQgNt7Wvdjtg",
        "outputId": "c1457d51-080d-4815-9af0-9f32c9391db5"
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Main settings\n",
        "epochs = 20\n",
        "embedding_dim = 100\n",
        "maxlen = 15   #100   #7\n",
        "output_file = 'output.txt'  # data/\n",
        "\n",
        "# Run grid search for each source (yelp, amazon, imdb)\n",
        "# for source, frame in df.groupby('source'):\n",
        "# print('Running grid search for data set :', source)\n",
        "sentences = data_set_train[\"clean_text\"].values  \n",
        "y = dummy_y\n",
        "# sentences = df['sentence'].values\n",
        "# y = df['label'].values\n",
        "\n",
        "# Train-test split\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "# Tokenize words\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "# Adding 1 because of reserved 0 index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Pad sequences with zeros\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Parameter grid for grid search\n",
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                  kernel_size=[3, 5, 7],\n",
        "                  vocab_size=[vocab_size],\n",
        "                  embedding_dim=[embedding_dim],\n",
        "                  maxlen=[maxlen])\n",
        "model = KerasClassifier(build_fn=create_model,\n",
        "                        epochs=epochs, batch_size=64,\n",
        "                        verbose=False)\n",
        "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=4, verbose=10, n_iter=5)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate testing set\n",
        "test_accuracy = grid.score(X_test, y_test)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=15, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=128, maxlen=15, kernel_size=3, embedding_dim=100, score=0.981, total=  37.9s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=15, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   37.9s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=128, maxlen=15, kernel_size=3, embedding_dim=100, score=0.986, total=  37.8s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=15, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.3min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=128, maxlen=15, kernel_size=3, embedding_dim=100, score=0.983, total=  38.0s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=15, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.9min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=128, maxlen=15, kernel_size=3, embedding_dim=100, score=0.983, total=  38.1s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=15, kernel_size=5, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  2.5min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=15, kernel_size=5, embedding_dim=100, score=0.980, total=  31.6s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=15, kernel_size=5, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=15, kernel_size=5, embedding_dim=100, score=0.986, total=  31.2s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=15, kernel_size=5, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  3.6min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=15, kernel_size=5, embedding_dim=100, score=0.981, total=  31.0s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=15, kernel_size=5, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  4.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=15, kernel_size=5, embedding_dim=100, score=0.985, total=  31.6s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=15, kernel_size=7, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  4.6min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=15, kernel_size=7, embedding_dim=100, score=0.978, total=  32.7s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=15, kernel_size=7, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  5.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=15, kernel_size=7, embedding_dim=100, score=0.985, total=  32.8s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=15, kernel_size=7, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=15, kernel_size=7, embedding_dim=100, score=0.982, total=  32.4s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=15, kernel_size=7, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=15, kernel_size=7, embedding_dim=100, score=0.986, total=  32.1s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=15, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=15, kernel_size=5, embedding_dim=100, score=0.978, total=  41.6s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=15, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=15, kernel_size=5, embedding_dim=100, score=0.986, total=  41.4s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=15, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=15, kernel_size=5, embedding_dim=100, score=0.985, total=  41.2s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=15, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=15, kernel_size=5, embedding_dim=100, score=0.987, total=  41.4s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=15, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=64, maxlen=15, kernel_size=5, embedding_dim=100, score=0.981, total=  33.8s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=15, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=64, maxlen=15, kernel_size=5, embedding_dim=100, score=0.986, total=  34.1s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=15, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=64, maxlen=15, kernel_size=5, embedding_dim=100, score=0.983, total=  34.5s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=15, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=64, maxlen=15, kernel_size=5, embedding_dim=100, score=0.985, total=  34.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 11.8min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-4YvcYdcIYd"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWs7iFbXk7uC",
        "outputId": "54931b59-85c9-49dd-bae6-266b557790fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "grid_result.best_score_"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9841365814208984"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKWqi5S6k7uc",
        "outputId": "6557fe19-22c2-4d55-ef3f-12b4865a109a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'embedding_dim': 100,\n",
              " 'kernel_size': 5,\n",
              " 'maxlen': 15,\n",
              " 'num_filters': 128,\n",
              " 'vocab_size': 9462}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D86RdrJIksKB"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtMKJbMqksGN",
        "outputId": "63a69387-fc85-4f44-81da-b19206c7a914",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Main settings\n",
        "epochs = 20\n",
        "embedding_dim = 100\n",
        "maxlen = 25   #100   #7\n",
        "output_file = 'output.txt'  # data/\n",
        "\n",
        "# Run grid search for each source (yelp, amazon, imdb)\n",
        "# for source, frame in df.groupby('source'):\n",
        "# print('Running grid search for data set :', source)\n",
        "sentences = data_set_train[\"clean_text\"].values  \n",
        "y = dummy_y\n",
        "# sentences = df['sentence'].values\n",
        "# y = df['label'].values\n",
        "\n",
        "# Train-test split\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "# Tokenize words\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "# Adding 1 because of reserved 0 index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Pad sequences with zeros\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Parameter grid for grid search\n",
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                  kernel_size=[3, 5, 7],\n",
        "                  vocab_size=[vocab_size],\n",
        "                  embedding_dim=[embedding_dim],\n",
        "                  maxlen=[maxlen])\n",
        "model = KerasClassifier(build_fn=create_model,\n",
        "                        epochs=epochs, batch_size=64,\n",
        "                        verbose=False)\n",
        "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=4, verbose=10, n_iter=5)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate testing set\n",
        "test_accuracy = grid.score(X_test, y_test)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=25, kernel_size=5, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=64, maxlen=25, kernel_size=5, embedding_dim=100, score=0.978, total=  44.3s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=25, kernel_size=5, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   44.3s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=64, maxlen=25, kernel_size=5, embedding_dim=100, score=0.986, total=  43.9s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=25, kernel_size=5, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.5min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=64, maxlen=25, kernel_size=5, embedding_dim=100, score=0.983, total=  43.7s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=25, kernel_size=5, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  2.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=64, maxlen=25, kernel_size=5, embedding_dim=100, score=0.986, total=  44.1s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=25, kernel_size=7, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  2.9min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=128, maxlen=25, kernel_size=7, embedding_dim=100, score=0.981, total= 1.1min\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=25, kernel_size=7, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=128, maxlen=25, kernel_size=7, embedding_dim=100, score=0.987, total= 1.1min\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=25, kernel_size=7, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  5.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=128, maxlen=25, kernel_size=7, embedding_dim=100, score=0.985, total= 1.0min\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=25, kernel_size=7, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  6.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=128, maxlen=25, kernel_size=7, embedding_dim=100, score=0.987, total= 1.0min\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=25, kernel_size=7, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  7.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=25, kernel_size=7, embedding_dim=100, score=0.981, total=  40.6s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=25, kernel_size=7, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  7.8min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=25, kernel_size=7, embedding_dim=100, score=0.985, total=  40.5s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=25, kernel_size=7, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=25, kernel_size=7, embedding_dim=100, score=0.983, total=  40.5s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=25, kernel_size=7, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=25, kernel_size=7, embedding_dim=100, score=0.984, total=  40.4s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=25, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=25, kernel_size=5, embedding_dim=100, score=0.979, total=  39.0s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=25, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=25, kernel_size=5, embedding_dim=100, score=0.985, total=  38.9s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=25, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=25, kernel_size=5, embedding_dim=100, score=0.985, total=  38.8s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=25, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=25, kernel_size=5, embedding_dim=100, score=0.985, total=  38.6s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=25, kernel_size=3, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=25, kernel_size=3, embedding_dim=100, score=0.981, total=  46.5s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=25, kernel_size=3, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=25, kernel_size=3, embedding_dim=100, score=0.987, total=  46.8s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=25, kernel_size=3, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=25, kernel_size=3, embedding_dim=100, score=0.984, total=  46.7s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=25, kernel_size=3, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=25, kernel_size=3, embedding_dim=100, score=0.985, total=  47.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 15.6min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koWX3ZEjksEC"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdvkS_X8k9PS",
        "outputId": "e4627f5c-40b8-4964-b04f-87ba199119e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "grid_result.best_score_"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9849102348089218"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W-V0ZL0k9PT",
        "outputId": "dfacb363-c013-4faf-8577-789ba3a1174a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'embedding_dim': 100,\n",
              " 'kernel_size': 7,\n",
              " 'maxlen': 25,\n",
              " 'num_filters': 128,\n",
              " 'vocab_size': 9462}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR9-e4y5ksBq"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02v9z4pNkr_X",
        "outputId": "d5c0c0e0-a293-4e21-f9d3-b70fc2125987",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Main settings\n",
        "epochs = 20\n",
        "embedding_dim = 100\n",
        "maxlen = 35   #100   #7\n",
        "output_file = 'output.txt'  # data/\n",
        "\n",
        "# Run grid search for each source (yelp, amazon, imdb)\n",
        "# for source, frame in df.groupby('source'):\n",
        "# print('Running grid search for data set :', source)\n",
        "sentences = data_set_train[\"clean_text\"].values  \n",
        "y = dummy_y\n",
        "# sentences = df['sentence'].values\n",
        "# y = df['label'].values\n",
        "\n",
        "# Train-test split\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "# Tokenize words\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "# Adding 1 because of reserved 0 index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Pad sequences with zeros\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Parameter grid for grid search\n",
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                  kernel_size=[3, 5, 7],\n",
        "                  vocab_size=[vocab_size],\n",
        "                  embedding_dim=[embedding_dim],\n",
        "                  maxlen=[maxlen])\n",
        "model = KerasClassifier(build_fn=create_model,\n",
        "                        epochs=epochs, batch_size=64,\n",
        "                        verbose=False)\n",
        "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=4, verbose=10, n_iter=5)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate testing set\n",
        "test_accuracy = grid.score(X_test, y_test)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=35, kernel_size=7, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=35, kernel_size=7, embedding_dim=100, score=0.980, total=  48.0s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=35, kernel_size=7, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   48.0s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=35, kernel_size=7, embedding_dim=100, score=0.986, total=  48.2s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=35, kernel_size=7, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.6min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=35, kernel_size=7, embedding_dim=100, score=0.985, total=  48.1s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=35, kernel_size=7, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  2.4min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=35, kernel_size=7, embedding_dim=100, score=0.985, total=  48.0s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=35, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  3.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=35, kernel_size=3, embedding_dim=100, score=0.983, total=  38.7s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=35, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.9min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=35, kernel_size=3, embedding_dim=100, score=0.986, total=  38.4s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=35, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  4.5min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=35, kernel_size=3, embedding_dim=100, score=0.986, total=  38.3s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=35, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  5.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=32, maxlen=35, kernel_size=3, embedding_dim=100, score=0.984, total=  38.3s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=35, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  5.8min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=128, maxlen=35, kernel_size=3, embedding_dim=100, score=0.980, total=  55.1s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=35, kernel_size=3, embedding_dim=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  6.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=128, maxlen=35, kernel_size=3, embedding_dim=100, score=0.987, total=  55.0s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=35, kernel_size=3, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=35, kernel_size=3, embedding_dim=100, score=0.984, total=  54.9s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=35, kernel_size=3, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=35, kernel_size=3, embedding_dim=100, score=0.985, total=  54.9s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=35, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=35, kernel_size=5, embedding_dim=100, score=0.979, total=  43.6s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=35, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=35, kernel_size=5, embedding_dim=100, score=0.986, total=  43.8s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=35, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=35, kernel_size=5, embedding_dim=100, score=0.981, total=  44.1s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=35, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=35, kernel_size=5, embedding_dim=100, score=0.985, total=  44.1s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=35, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=35, kernel_size=5, embedding_dim=100, score=0.981, total= 1.1min\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=35, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=35, kernel_size=5, embedding_dim=100, score=0.986, total= 1.1min\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=35, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=35, kernel_size=5, embedding_dim=100, score=0.985, total= 1.2min\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=35, kernel_size=5, embedding_dim=100 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=35, kernel_size=5, embedding_dim=100, score=0.986, total= 1.1min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 16.9min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AanF0WY7k4pb"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFtMe2vJk-Mz",
        "outputId": "15715490-43d4-49ce-f2c7-b73a68a793ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "grid_result.best_score_"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.984813392162323"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZhnWQSIk-M0",
        "outputId": "6334ec3e-082a-4e2e-9079-4f708475dbb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'embedding_dim': 100,\n",
              " 'kernel_size': 3,\n",
              " 'maxlen': 35,\n",
              " 'num_filters': 32,\n",
              " 'vocab_size': 9462}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDKlKx1akr82"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0IdSKiJtzgd"
      },
      "source": [
        "#### Predykcja na zbiorze validacyjnym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27dr_LCO_1IW",
        "outputId": "eae06113-85a5-4a63-b9fc-ba7da6935966"
      },
      "source": [
        "# oczysczenie danych\n",
        "data_set_valid[\"clean_text\"] = data_set_valid[\"text\"].apply(lambda x: process_text(x))\n",
        "\n",
        "# labelencoder \n",
        "data_set_valid[\"labelencoder\"] = labelencoder.fit_transform(data_set_valid[\"label\"])\n",
        "\n",
        "# tokenizacja weg przetrenowanego już tokenizera\n",
        "X_validate = tokenizer.texts_to_sequences(data_set_valid[\"clean_text\"])\n",
        "\n",
        "# pad sequel\n",
        "X_validate = pad_sequences(X_validate, padding=\"post\", truncating=\"post\", maxlen=maxlen)\n",
        "X_validate"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 16,  42,  53, ...,   0,   0,   0],\n",
              "       [250, 176,  21, ...,   0,   0,   0],\n",
              "       [ 16,   9,   1, ...,   0,   0,   0],\n",
              "       ...,\n",
              "       [ 27,   1,  84, ...,   0,   0,   0],\n",
              "       [ 16,   9,   1, ...,   0,   0,   0],\n",
              "       [ 44,  40,   4, ...,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOPVqktyANNF",
        "outputId": "7afa619e-2b36-436e-c0ae-348817c4e45e"
      },
      "source": [
        "dummy_y_valid = data_set_valid[\"labelencoder\"].values\n",
        "dummy_y_valid[:5]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6, 3, 2, 0, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAiFC304APqL",
        "outputId": "805ef87c-640a-4cd4-8404-3e8296d12f0b"
      },
      "source": [
        "# Sprawdzenie rozmiaru zbiorów validacyjnego\n",
        "X_validate.shape, dummy_y_valid.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((700, 35), (700,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "WRbEdb7AAXGy",
        "outputId": "60f1450c-d57c-4402-9586-67b629207017"
      },
      "source": [
        "predicted_lstm_val = np.argmax(model.predict(X_validate), axis=-1)\n",
        "predicted_lstm_val[:5]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-ce89075373f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted_lstm_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredicted_lstm_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \"\"\"\n\u001b[1;32m    240\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_sk_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KerasClassifier' object has no attribute 'model'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5wZwn-_uKE9"
      },
      "source": [
        "#### Rozkodowanie przewidzianych i prawidłowych etykiet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4U9hYvnBJ9F"
      },
      "source": [
        "y_pred = labelencoder.inverse_transform(predicted_lstm_val)\n",
        "y_pred = pd.Series(y_pred)\n",
        "y_pred.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW03pUanA729"
      },
      "source": [
        "y_val = labelencoder.inverse_transform(dummy_y_valid)\n",
        "y_val = pd.Series(y_val)\n",
        "y_val.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcGfyRgQudTZ"
      },
      "source": [
        "#### `Confusion matrix`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXVwswIsA9RT"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGr6-cjzBFLc"
      },
      "source": [
        "classes = np.unique(y_val)\n",
        "\n",
        "print('Accuracy:', round(accuracy_score(y_val, y_pred),2))\n",
        "print('F1_score:', round(f1_score(y_val, y_pred, average='weighted'),2))\n",
        "\n",
        "print(classification_report(y_val, y_pred))\n",
        "# Plot confusion matrix\n",
        "cm = confusion_matrix(y_val, y_pred)\n",
        "fig, ax = plt.subplots()\n",
        "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, cbar=False)\n",
        "ax.set(xlabel='Pred', ylabel='True', xticklabels=classes, yticklabels=classes, title='Confusion matrix')\n",
        "plt.yticks(rotation=0)\n",
        "plt.xticks(rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uxfqF9uum32"
      },
      "source": [
        "#### Zapoznanie się z błędnymi predykcjami "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28QYnOPM-mJz"
      },
      "source": [
        "indexes = []\n",
        "for i, phrase in enumerate(y_pred):\n",
        "  if phrase == 'SearchCreativeWork':\n",
        "    if y_val[i] == 'SearchScreeningEvent':\n",
        "      indexes.append(i)\n",
        "print(indexes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouy1yzy8-048"
      },
      "source": [
        "for i in indexes:\n",
        "  print(f\"----------------------------\\nTekst komendy:\\n{data_set_valid['text'][i]}\")\n",
        "  print(f\"Oczyszczona komenda:\\n{data_set_valid['clean_text'][i]}\")\n",
        "  print(f'True category: {y_val[i]}')\n",
        "  print(f'Predicted category: {y_pred[i]}')\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm61MY9CSpLm"
      },
      "source": [
        "Wnioski:\n",
        "> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klnp99BGNgm8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}