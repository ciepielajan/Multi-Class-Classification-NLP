{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detecting_intentions_MLP",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMq5NSTNqHfJu8Es7CwgQqf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ciepielajan/Multi-Class-Classification-NLP/blob/main/Detecting_intentions_CNN_KerasClassifier_i_RandomizedSearchCV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fImOu-ux11_s"
      },
      "source": [
        "KerasClassifier i Randomized Search Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us7PJ6w-n8ge"
      },
      "source": [
        "#### Pobranie danych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ_7ET22CAsA",
        "outputId": "28dcb159-985e-405f-fff5-e91b84acd278"
      },
      "source": [
        "# https://drive.google.com/file/d/1fI6EXyD9TMTC1jzdu206ljXOGNjdHprq/view?usp=sharing\n",
        "!gdown --id \"1fI6EXyD9TMTC1jzdu206ljXOGNjdHprq\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fI6EXyD9TMTC1jzdu206ljXOGNjdHprq\n",
            "To: /content/user_intent.zip\n",
            "\r  0% 0.00/271k [00:00<?, ?B/s]\r100% 271k/271k [00:00<00:00, 37.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWusnzuPCHqv",
        "outputId": "cd97b5fe-5966-4f6a-f333-81931d89c35d"
      },
      "source": [
        "!unzip \"user_intent.zip\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  user_intent.zip\n",
            "  inflating: train.csv               \n",
            "  inflating: __MACOSX/._train.csv    \n",
            "  inflating: validation.csv          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CI1LZgj-Kin"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUKeVYWfCOdU",
        "outputId": "c963d081-2b9b-4b89-e2fb-ae3a7e835cad"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "data_set_train = pd.read_csv(\"train.csv\")\n",
        "data_set_valid = pd.read_csv(\"validation.csv\")\n",
        "\n",
        "print(data_set_train.shape)\n",
        "print(data_set_valid.shape)\n",
        "\n",
        "print(data_set_train.columns)\n",
        "print(data_set_valid.columns)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13784, 2)\n",
            "(700, 2)\n",
            "Index(['text', 'label'], dtype='object')\n",
            "Index(['text', 'label'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vg1fqbGoCrp"
      },
      "source": [
        "#### Podstawowy process text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GQNuCjSlz0F"
      },
      "source": [
        "import re\n",
        "def process_text(sentence):\n",
        "    sentence = re.sub('[A-Za-z0-9]+@[a-zA-z].[a-zA-Z]+', '', sentence)  # maile\n",
        "    sentence = re.sub('(http[s]*:[/][/])[a-zA-Z0-9]+', '', sentence)  # linki\n",
        "    sentence = re.sub(r\"<[^>]+>\", \" \", sentence) # remove html tag\n",
        "    sentence = re.sub(r\"[^a-zA-Z\\s]\", \"\", sentence)  # remove punctuations and numbers   \n",
        "    sentence = re.sub(r\"\\b[A-Za-z]{1}\\b\", \"\", sentence)  # remove single characters\n",
        "\n",
        "    sentence = re.sub(\"^\\s+|\\s+$\", \"\", sentence, flags=re.UNICODE) # Remove spaces both in the BEGINNING and in the END of a string:\n",
        "    sentence = \" \".join(re.split(\"\\s+\", sentence, flags=re.UNICODE))  # Remove ONLY DUPLICATE spaces:\n",
        "    sentence = sentence.lower()\n",
        "    \n",
        "    return sentence"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZJOju1sFOwp"
      },
      "source": [
        "data_set_train[\"clean_text\"] = data_set_train[\"text\"].apply(lambda x: process_text(x))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7epaW5cXoXef"
      },
      "source": [
        "#### `LabelEncoder` oraz `to_categorical`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "022rNoaL-xbl",
        "outputId": "ad8cb3c3-21d8-430f-b19d-9f16600a0f4b"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical \n",
        "\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "data_set_train[\"labelencoder\"] = labelencoder.fit_transform(data_set_train[\"label\"])\n",
        "\n",
        "dummy_y = to_categorical(data_set_train[\"labelencoder\"], dtype =\"float32\")\n",
        "\n",
        "data_set_train[[\"clean_text\",\"label\",\"labelencoder\"]].head(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_text</th>\n",
              "      <th>label</th>\n",
              "      <th>labelencoder</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>find cinema nearest for films</td>\n",
              "      <td>SearchScreeningEvent</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>give the current series two stars</td>\n",
              "      <td>RateBook</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>find the good girl at movie house</td>\n",
              "      <td>SearchScreeningEvent</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>please make reservations for three at kosher t...</td>\n",
              "      <td>BookRestaurant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what is the forecast for here one second from now</td>\n",
              "      <td>GetWeather</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          clean_text  ... labelencoder\n",
              "0                      find cinema nearest for films  ...            6\n",
              "1                  give the current series two stars  ...            4\n",
              "2                  find the good girl at movie house  ...            6\n",
              "3  please make reservations for three at kosher t...  ...            1\n",
              "4  what is the forecast for here one second from now  ...            2\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RfsD1LSpSag",
        "outputId": "0b044ede-99ee-4070-ecc0-4c6f031affe6"
      },
      "source": [
        "dummy_y.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13784, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4dq1macoeQC",
        "outputId": "e12e7035-f58c-4d6e-8fd4-2cc48d8a46e2"
      },
      "source": [
        "id_intention = 6\n",
        "print(\"Sprawdzenie poprawności LabelEncoder i to_categorical \\n\")\n",
        "print(\"Label - \", data_set_train[\"label\"].iloc[id_intention])\n",
        "print(\"LabelEncoder - \", data_set_train[\"labelencoder\"].iloc[id_intention])\n",
        "print()\n",
        "print(\"to_categorical - \", dummy_y[id_intention])\n",
        "print()\n",
        "print(\"return to LabelEncoder - \",np.argmax(dummy_y[id_intention], axis=-1))\n",
        "print(\"return to Label - \",labelencoder.inverse_transform([np.argmax(dummy_y[id_intention], axis=-1)]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sprawdzenie poprawności LabelEncoder i to_categorical \n",
            "\n",
            "Label -  BookRestaurant\n",
            "LabelEncoder -  1\n",
            "\n",
            "to_categorical -  [0. 1. 0. 0. 0. 0. 0.]\n",
            "\n",
            "return to LabelEncoder -  1\n",
            "return to Label -  ['BookRestaurant']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lZTDFgGoSmT"
      },
      "source": [
        "#### Zdefiniowanie X i y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWd9mEjfBAS2"
      },
      "source": [
        "X = data_set_train[\"clean_text\"]\n",
        "y = dummy_y"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jUlcX2npzFs",
        "outputId": "ac4fadda-82e1-4496-bb1b-c7bbc9c994f1"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((13784,), (13784, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UAQ4RNONmda"
      },
      "source": [
        "\n",
        "#### `KerasClassifier` i `Randomized Search Cross Validation`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY8E6QMDMpHS"
      },
      "source": [
        "`Randomized Search Cross Validation`\n",
        "\n",
        "Wykorzystuje to losowy zestaw hiperparametrów. Przydatne, gdy istnieje wiele hiperparametrów, więc przestrzeń wyszukiwania jest duża. Może być używany, jeśli masz wcześniejsze przekonanie o tym, jakie powinny być hiperparametry. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A18D02x5sjFd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCm-rbaus6QG"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaRWehbu_VCe"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwih4PgGsS9k"
      },
      "source": [
        "from keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPooling1D, Dropout, Conv1D"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnfPMuzQ-Cnq"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOcctLFPuKoa"
      },
      "source": [
        "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "    model.add(Conv1D(num_filters, kernel_size, activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(7, activation=\"softmax\")) \n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4tXfrhOxi7E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fOG5tkSuR0A",
        "outputId": "4474662f-5222-4377-bba2-ea36ccb1c806"
      },
      "source": [
        "\n",
        "\n",
        "# df z wynikami wszystkich RandomizedSearchCV\n",
        "wyniki = pd.DataFrame()\n",
        "wyniki\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "\n",
        "# Main settings\n",
        "epochs = 20\n",
        "embedding_dim = 100\n",
        "maxlen = 13\n",
        "\n",
        "# Run grid search for each source (yelp, amazon, imdb)\n",
        "# for source, frame in df.groupby('source'):\n",
        "# print('Running grid search for data set :', source)\n",
        "sentences = data_set_train[\"clean_text\"].values  \n",
        "y = dummy_y\n",
        "\n",
        "# Train-test split\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "# Tokenize words\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "# Adding 1 because of reserved 0 index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Pad sequences with zeros\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Parameter grid for grid search\n",
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                  kernel_size=[3, 5, 7],\n",
        "                  vocab_size=[vocab_size],\n",
        "                  embedding_dim=[100,200],\n",
        "                  maxlen=[maxlen],\n",
        "                  epochs = [20],\n",
        "                  batch_size=[16,32,64] )\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model,\n",
        "                        verbose=False)\n",
        "\n",
        "\n",
        "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=4, verbose=10, n_iter=5)  #n_liter odpowiada za ilość radomize serach\n",
        "# grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "grid_result = grid.fit(X_train, y_train, validation_split=0.2, callbacks=[early_stopping])  #validation_split=0.2\n",
        "\n",
        "# Evaluate testing set\n",
        "test_accuracy = grid.score(X_test, y_test)\n",
        "grid_result.best_params_.update({\"best_score_\":grid_result.best_score_})\n",
        "wyniki = wyniki.append(grid_result.best_params_, ignore_index=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=13, kernel_size=5, epochs=20, embedding_dim=100, batch_size=32 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=64, maxlen=13, kernel_size=5, epochs=20, embedding_dim=100, batch_size=32, score=0.978, total=  32.6s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=13, kernel_size=5, epochs=20, embedding_dim=100, batch_size=32 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   32.6s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=64, maxlen=13, kernel_size=5, epochs=20, embedding_dim=100, batch_size=32, score=0.983, total=  31.4s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=13, kernel_size=5, epochs=20, embedding_dim=100, batch_size=32 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=64, maxlen=13, kernel_size=5, epochs=20, embedding_dim=100, batch_size=32, score=0.982, total=  26.0s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=13, kernel_size=5, epochs=20, embedding_dim=100, batch_size=32 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.5min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=64, maxlen=13, kernel_size=5, epochs=20, embedding_dim=100, batch_size=32, score=0.984, total=  31.0s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=13, kernel_size=3, epochs=20, embedding_dim=100, batch_size=32 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  2.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=64, maxlen=13, kernel_size=3, epochs=20, embedding_dim=100, batch_size=32, score=0.978, total=  30.7s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=13, kernel_size=3, epochs=20, embedding_dim=100, batch_size=32 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.5min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=64, maxlen=13, kernel_size=3, epochs=20, embedding_dim=100, batch_size=32, score=0.982, total=  28.1s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=13, kernel_size=3, epochs=20, embedding_dim=100, batch_size=32 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  3.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=64, maxlen=13, kernel_size=3, epochs=20, embedding_dim=100, batch_size=32, score=0.983, total=  26.0s\n",
            "[CV] vocab_size=9462, num_filters=64, maxlen=13, kernel_size=3, epochs=20, embedding_dim=100, batch_size=32 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  3.4min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=64, maxlen=13, kernel_size=3, epochs=20, embedding_dim=100, batch_size=32, score=0.984, total=  33.1s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=13, kernel_size=7, epochs=20, embedding_dim=100, batch_size=16 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  4.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=128, maxlen=13, kernel_size=7, epochs=20, embedding_dim=100, batch_size=16, score=0.978, total=  40.9s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=13, kernel_size=7, epochs=20, embedding_dim=100, batch_size=16 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  4.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  vocab_size=9462, num_filters=128, maxlen=13, kernel_size=7, epochs=20, embedding_dim=100, batch_size=16, score=0.986, total=  52.8s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=13, kernel_size=7, epochs=20, embedding_dim=100, batch_size=16 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=13, kernel_size=7, epochs=20, embedding_dim=100, batch_size=16, score=0.982, total=  42.5s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=13, kernel_size=7, epochs=20, embedding_dim=100, batch_size=16 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=13, kernel_size=7, epochs=20, embedding_dim=100, batch_size=16, score=0.985, total=  55.9s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=13, kernel_size=3, epochs=20, embedding_dim=200, batch_size=32 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=13, kernel_size=3, epochs=20, embedding_dim=200, batch_size=32, score=0.978, total=  50.4s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=13, kernel_size=3, epochs=20, embedding_dim=200, batch_size=32 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=13, kernel_size=3, epochs=20, embedding_dim=200, batch_size=32, score=0.985, total=  50.2s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=13, kernel_size=3, epochs=20, embedding_dim=200, batch_size=32 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=13, kernel_size=3, epochs=20, embedding_dim=200, batch_size=32, score=0.983, total=  45.4s\n",
            "[CV] vocab_size=9462, num_filters=128, maxlen=13, kernel_size=3, epochs=20, embedding_dim=200, batch_size=32 \n",
            "[CV]  vocab_size=9462, num_filters=128, maxlen=13, kernel_size=3, epochs=20, embedding_dim=200, batch_size=32, score=0.985, total=  59.9s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=13, kernel_size=7, epochs=20, embedding_dim=100, batch_size=16 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=13, kernel_size=7, epochs=20, embedding_dim=100, batch_size=16, score=0.979, total= 1.0min\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=13, kernel_size=7, epochs=20, embedding_dim=100, batch_size=16 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=13, kernel_size=7, epochs=20, embedding_dim=100, batch_size=16, score=0.985, total=  36.8s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=13, kernel_size=7, epochs=20, embedding_dim=100, batch_size=16 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=13, kernel_size=7, epochs=20, embedding_dim=100, batch_size=16, score=0.981, total=  41.5s\n",
            "[CV] vocab_size=9462, num_filters=32, maxlen=13, kernel_size=7, epochs=20, embedding_dim=100, batch_size=16 \n",
            "[CV]  vocab_size=9462, num_filters=32, maxlen=13, kernel_size=7, epochs=20, embedding_dim=100, batch_size=16, score=0.983, total=  54.4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 13.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 25min 23s, sys: 1min 18s, total: 26min 41s\n",
            "Wall time: 14min 59s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "lksp-GnoA-Hf",
        "outputId": "154e28cb-a8a5-4c63-e1f6-967241b7a3b8"
      },
      "source": [
        "wyniki"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>batch_size</th>\n",
              "      <th>best_score_</th>\n",
              "      <th>embedding_dim</th>\n",
              "      <th>epochs</th>\n",
              "      <th>kernel_size</th>\n",
              "      <th>maxlen</th>\n",
              "      <th>num_filters</th>\n",
              "      <th>vocab_size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>32.0</td>\n",
              "      <td>0.982879</td>\n",
              "      <td>200.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>9462.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   batch_size  best_score_  embedding_dim  ...  maxlen  num_filters  vocab_size\n",
              "0        32.0     0.982879          200.0  ...    13.0        128.0      9462.0\n",
              "\n",
              "[1 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvJ8G7GU5Oe-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41abf00a-1e63-4c0b-b69a-87c84300f3b2"
      },
      "source": [
        "print(\"#### RandomizedSearchCV\")\n",
        "print(wyniki.to_markdown())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#### RandomizedSearchCV\n",
            "|    |   batch_size |   best_score_ |   embedding_dim |   epochs |   kernel_size |   maxlen |   num_filters |   vocab_size |\n",
            "|---:|-------------:|--------------:|----------------:|---------:|--------------:|---------:|--------------:|-------------:|\n",
            "|  0 |           32 |      0.982879 |             200 |       20 |             3 |       13 |           128 |         9462 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfXA7iRkcCNl"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATmvi7m9cCK6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vioWd2xMxru"
      },
      "source": [
        "`Grid Search Cross Validation`\n",
        "\n",
        "Tworzy siatkę nad przestrzenią wyszukiwania i ocenia model pod kątem wszystkich możliwych hiperparametrów w przestrzeni. Dobre w tym sensie, że jest proste i wyczerpujące. Z drugiej strony, może to być zbyt kosztowne w czasie obliczeń, jeśli przestrzeń poszukiwań jest duża (np. Bardzo wiele hiperparametrów).\n",
        "\n",
        "Przykład do zastosowania: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M23wynAYTodU"
      },
      "source": [
        "%%time\n",
        "\n",
        "# df z wynikami wszystkich RandomizedSearchCV\n",
        "wyniki = pd.DataFrame()\n",
        "wyniki\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier  \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# Main settings\n",
        "# epochs = 20\n",
        "# embedding_dim = 100\n",
        "maxlen = 13\n",
        "\n",
        "# Run grid search for each source (yelp, amazon, imdb)\n",
        "# for source, frame in df.groupby('source'):\n",
        "# print('Running grid search for data set :', source)\n",
        "sentences = data_set_train[\"clean_text\"].values  \n",
        "y = dummy_y\n",
        "\n",
        "# Train-test split\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "# Tokenize words\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "# Adding 1 because of reserved 0 index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Pad sequences with zeros\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Parameter grid for grid search\n",
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                  kernel_size=[3, 5, 7],\n",
        "                  vocab_size=[vocab_size],\n",
        "                  embedding_dim=[100,200],\n",
        "                  maxlen=[maxlen],\n",
        "                  epochs = [30],   \n",
        "                  batch_size=[16,32,64]\n",
        "                  )\n",
        "\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model,verbose=False)\n",
        "\n",
        "grid = GridSearchCV(estimator=model , param_grid=param_grid, n_jobs=-1, cv=3, verbose=10)\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "grid_result = grid.fit(X_train, y_train, validation_split=0.2, callbacks=[early_stopping])  #validation_split=0.2\n",
        "\n",
        "# Evaluate testing set\n",
        "test_accuracy = grid.score(X_test, y_test)\n",
        "grid_result.best_params_.update({\"best_score_\":grid_result.best_score_})\n",
        "wyniki = wyniki.append(grid_result.best_params_, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKQsYW_VTodZ"
      },
      "source": [
        "wyniki"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GNSbENU2NKh"
      },
      "source": [
        "print(\"#### RandomizedSearchCV\")\n",
        "print(wyniki.to_markdown())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0IdSKiJtzgd"
      },
      "source": [
        "#### Predykcja na zbiorze validacyjnym"
      ]
    }
  ]
}